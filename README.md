# Introduction
### What are Transformers?
Transformers are a groundbreaking model architecture introduced in the field of natural language processing (NLP) by Vaswani et al. in 2017. They have since transformed the landscape of machine learning by enabling state-of-the-art results not only in NLP but also in computer vision, reinforcement learning, and other domains. The core idea behind Transformers is the use of self-attention mechanisms, which allow the model to weigh the importance of each part of the input sequence and build contextualized representations effectively. This architecture has paved the way for highly parallelizable models, resulting in faster training and the ability to process much larger datasets.

In this repository, we’ll dive into the foundational concepts of Transformers, explore their architecture and components, and implement various Transformer models step by step. We’ll start with basic attention mechanisms, understand positional encodings, and build up to advanced models like BERT, GPT, and beyond. By the end of this journey, you'll not only understand how Transformers work but also gain the practical skills to build and fine-tune them for your own projects.

# Contents Overview
This repository is designed to guide you through the journey of understanding and implementing Transformer models. We begin with the fundamental concepts of Transformer architecture and attention mechanisms, explore the distinction between foundation and fine-tuned models, and delve into the immense impact Transformers have made across fields. As you proceed, you’ll encounter practical applications, a detailed catalog of notable models, and insights into related architectures like diffusion models.

Here’s a complete list of the contents that will be in this repository:
**Introduction**
01. **What are Transformers**  
02. **Encoder/Decoder Architecture**  
03. **Attention Mechanisms**  
04. **Foundation vs Fine-tuned Models**  
05. **The Impact of Transformers**  
06. **A Note on Diffusion Models**  
07. **The Transformers Catalog**  
08. **Key Features of a Transformer**  
09. **Pretraining Architecture**  
10. **Pretraining or Finetuning Task**  
11. **Applications of Transformers**  
12. **Catalog Table**  
13. **Family Tree of Transformers**  
14. **Chronological Timeline of Developments**  
15. **Comprehensive Catalog List**  

**Detailed Model Descriptions and Analysis**  
16. **ALBERT**  
17. **AlexaTM 20B**  
18. **Alpaca**  
19. **AlphaFold**  
20. **Anthropic Assistant**  
21. **BART**  
22. **BERT**  
23. **Big Bird**  
24. **BlenderBot3**  
25. **BLOOM**  
26. **ChatGPT**  
27. **Chinchilla**  
28. **CLIP**  
29. **CM3**  
30. **CTRL**  
31. **DALL-E**  
32. **DALL-E 2**  
33. **DeBERTa**  
34. **Decision Transformers**  
35. **DialoGPT**  
36. **DistilBERT**  
37. **DQ-BART**  
38. **Dolly**  
39. **E5**  
40. **ELECTRA**  
41. **ERNIE**  
42. **Flamingo**  
43. **Flan-T5**  
44. **Flan-PaLM**  
45. **Galactica**  
46. **Gato**  
47. **GLaM**  
48. **GLIDE**  
49. **GLM**  
50. **Global Context ViT**  
51. **Gopher**  
52. **GopherCite**  
53. **GPT**  
54. **GPT-2**  
55. **GPT-3**  
56. **GPT-3.5**  
57. **GPT-J**  
58. **GPT-Neo**  
59. **GPT-NeoX-20B**  
60. **HTLM**  
61. **Imagen**  
62. **InstructGPT**  
63. **InstructOR**  
64. **Jurassic-1**  
65. **LAMDA**  
66. **LLaMA**  
67. **mBART**  
68. **Megatron**  
69. **Minerva**  
70. **MT-NLG (Megatron TuringNLG)**  
71. **OpenAssistant LLaMa**  
72. **OPT**  
73. **PalM**  
74. **Pegasus**  
75. **Pythia**  
76. **RoBERTa**  
77. **SeeKer**  
78. **Sparrow**  
79. **StableDiffusion**  
80. **Swin Transformer**  
81. **Switch**  
82. **T0**  
83. **T5**  
84. **Trajectory Transformers**  
85. **Transformer XL**  
86. **Turing-NLG**  
87. **UL2**  
88. **Vicuna**  
89. **ViT**  
90. **Wu Dao 2.0**  
91. **XLM-RoBERTa**  
92. **XLNet**
